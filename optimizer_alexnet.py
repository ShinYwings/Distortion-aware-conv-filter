# pylint: disable=g-classes-have-attributes
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.python.framework import ops
from tensorflow.python.framework import constant_op
from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import resource_variable_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.training import training_ops
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.util.tf_export import keras_export
from tensorflow.python.keras import backend_config
from tensorflow.python.keras import backend as K
import numpy as np
import tensorflow as tf

@keras_export("keras.optimizers.schedules.AlexNetLRSchedule")
class AlexNetLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

    def __init__(self, initial_learning_rate = "initial_learning_rate",  name="AlexNetLRSchedule"):
        
        super(AlexNetLRSchedule, self).__init__()
        
        self.initial_learning_rate = initial_learning_rate
        self.decay_rate = tf.Variable(0., trainable=False)
        self.decay_termination = False
        self.name = name
        
    def cnt_up_num_of_statinary_loss(self):
        self.decay_rate.assign_add(1., use_locking=True)
    
    def turn_on_last_epoch_loss(self):
        self.decay_termination = True

    def __call__(self, step):
        with ops.name_scope_v2(self.name or "AlexNetLRSchedule"):
            
            initial_learning_rate = ops.convert_to_tensor_v2(
                self.initial_learning_rate, name="initial_learning_rate")
            dtype = initial_learning_rate.dtype
            global_step =  math_ops.cast(step, dtype)

            alexnet_decay = math_ops.pow(10.,self.decay_rate)
            tt = math_ops.cast(alexnet_decay, dtype)
            
            # If exclusive==False, execution stops at the first predicate which evaluates to True,
            # and the tensors generated by the corresponding function are returned immediately.
            # If none of the predicates evaluate to True, 
            # this operation returns the tensors generated by default.
            # TODO 종료전 3배 감소 근데 termination의 정의가 헷갈림...     
            def op_last_step_decay(): return math_ops.div(initial_learning_rate, 3.0*tt)
            def op_loss_stationary_decay(): return math_ops.div(initial_learning_rate, tt)
            return control_flow_ops.cond(tf.equal(self.decay_termination, True), op_last_step_decay, op_loss_stationary_decay)
            
            # return math_ops.div(initial_learning_rate, tt)
    
    def get_config(self):
        return {
            'initial_learning_rate' : self.initial_learning_rate,
            'decay_rate' : self.decay_rate,
            'name' : self.name
        }

@keras_export('keras.optimizers.AlexSGD')
class AlexSGD(OptimizerV2):

    # Subclasses should set this to True unless they override `apply_gradients`
    # with a version that does not have the `experimental_aggregate_gradients`
    # argument.  Older versions of Keras did not have this argument so custom
    # optimizers may have overridden `apply_gradients` without the
    # `experimental_aggregate_gradients` argument. Keras only passes
    # `experimental_aggregate_gradients` if this attribute is True.
    # Note: This attribute will likely be removed in an upcoming release.
    _HAS_AGGREGATE_GRAD = True
    
    def __init__(self,
                learning_rate="learning_rate",
                momentum=0.9,
                weight_decay=5e-4,
                name="AlexSGD",
                **kwargs):
                
        super(AlexSGD, self).__init__(name,**kwargs)
        self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
        self._set_hyper("decay", self._initial_decay)
        self._is_first = True
        self._set_hyper("momentum", momentum)
        self._set_hyper("weight_decay", weight_decay)

    # TODO 바꾼거
    @classmethod
    def from_config(cls, config):
        custom_objects = {'AlexNetLRSchedule': AlexNetLRSchedule}
        return super(AlexSGD, cls).from_config(config, custom_objects=custom_objects)

    def _create_slots(self, var_list):

        for var in var_list:
            self.add_slot(var, 'v')
        
    def _resource_apply_dense(self, grad, var):

        var_device, var_dtype = var.device, var.dtype.base_dtype
        # coefficients = ((apply_state or {}).get((var_device, var_dtype))
        #                 or self._fallback_apply_state(var_device, var_dtype))
        
        lr_var = self._decayed_lr(var_dtype)
        lr_t = math_ops.cast(lr_var, var_dtype)

        momentum_var = array_ops.identity(self._get_hyper('momentum', var_dtype))
        weight_decay_var = array_ops.identity(self._get_hyper('weight_decay', var_dtype))    
        v = self.get_slot(var, "v")
        
        if self._is_first:
            self._is_first = False

            # TODO 왜 아직도 v를 초기화 안시켜줘도 되는지 이해 못하겠다.
            # v = state_ops.assign(v , 1, use_locking=self._use_locking)
            # v_var = state_ops.assign(v, 1)
            left = math_ops.mul(momentum_var,v)
            center_1 = math_ops.mul(weight_decay_var, lr_t)

            center_2 = center_1 * var
            right = math_ops.mul(lr_t, grad)
            sub_1 = left * center_2
            v_t = state_ops.assign(v, math_ops.subtract(sub_1, right), use_locking=self._use_locking)
            var_update = state_ops.assign(var, var+v_t, use_locking=self._use_locking)
            
        else:
            
            left = math_ops.mul(momentum_var,v)
            center_1 = math_ops.mul(weight_decay_var, lr_t)
            center_2 = center_1 * var
            right = math_ops.mul(lr_t, grad)
            sub_1 = left * center_2
            v_t = state_ops.assign(v, math_ops.subtract(sub_1, right) , use_locking=self._use_locking)
            var_update = state_ops.assign(var, var+v_t, use_locking=self._use_locking)
            
        updates = [var_update, v_t]

        # with ops.control_dependencies(updates):
        return control_flow_ops.group(*updates)
        
    def _resource_apply_sparse(self, grad, var, apply_state=None):
        raise NotImplementedError("Sparse gradient updates are not supported.")

    def get_config(self):
      config = super(AlexSGD, self).get_config()
      config.update({
          "learning_rate": self._serialize_hyperparameter("learning_rate"),
          "decay": self._serialize_hyperparameter("decay"),
          "v": self._serialize_hyperparameter("v"),
          "momentum": self._serialize_hyperparameter("momentum"),
          "weight_decay": self._serialize_hyperparameter("weight_decay"),
          "is_first": self._is_first
      })
      return config